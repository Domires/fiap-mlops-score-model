# MODELO ÃšNICO DE CREDIT SCORE - RANDOM FOREST
# Este script treina APENAS 1 MODELO Random Forest para classificaÃ§Ã£o de score de crÃ©dito

import pandas as pd
import numpy as np
import joblib
import os
import json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

print("âœ… Bibliotecas carregadas - APENAS para Random Forest!")

# =============================================================================
# 1. CARREGAMENTO DOS DADOS
# =============================================================================

train_df = pd.read_csv('references/exemplo_train.csv')
test_df = pd.read_csv('references/exemplo_test.csv')

print(f"ğŸ“Š Dados de treino: {train_df.shape}")
print(f"ğŸ“Š Dados de teste: {test_df.shape}")
print(f"\nğŸ¯ Classes Ãºnicas no target: {train_df['Credit_Score'].unique()}")

# VerificaÃ§Ã£o de dados mÃ­nimos
if len(train_df) < 10:
    print("âš ï¸ AVISO: Conjunto de dados muito pequeno para ML robusto!")
    print("ğŸ’¡ RecomendaÃ§Ã£o: Use dados maiores para melhor performance")
if len(train_df['Credit_Score'].unique()) < 2:
    print("âŒ ERRO: Precisa de pelo menos 2 classes para classificaÃ§Ã£o!")
    exit(1)

# =============================================================================
# 2. PRÃ‰-PROCESSAMENTO SIMPLES E EFICAZ
# =============================================================================

def preprocess_simple(df):
    """PrÃ©-processamento simplificado para evitar erros"""
    df_clean = df.copy()
    
    # Remove colunas desnecessÃ¡rias
    cols_to_drop = ['ID', 'Customer_ID', 'Name', 'SSN']
    df_clean = df_clean.drop(columns=[col for col in cols_to_drop if col in df_clean.columns])
    
    # Converte colunas numÃ©ricas
    numeric_cols = ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 
                   'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date',
                   'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries',
                   'Outstanding_Debt', 'Credit_Utilization_Ratio', 'Total_EMI_per_month',
                   'Amount_invested_monthly', 'Monthly_Balance']
    
    for col in numeric_cols:
        if col in df_clean.columns:
            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
    
    # Limpa valores problemÃ¡ticos em colunas categÃ³ricas
    categorical_cols = ['Month', 'Occupation', 'Type_of_Loan', 'Credit_Mix', 
                       'Credit_History_Age', 'Payment_of_Min_Amount', 'Payment_Behaviour']
    
    for col in categorical_cols:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].astype(str)
            df_clean[col] = df_clean[col].replace(['_', '!@9#%8', '#F%$D@*&8', '_______', 'NA', 'nan'], 'Unknown')
    
    return df_clean

# Aplica prÃ©-processamento
train_clean = preprocess_simple(train_df)
test_clean = preprocess_simple(test_df)

print("âœ… PrÃ©-processamento concluÃ­do!")
print(f"ğŸ“Š Treino limpo: {train_clean.shape}")
print(f"ğŸ“Š Teste limpo: {test_clean.shape}")

# =============================================================================
# 3. PREPARAÃ‡ÃƒO DAS FEATURES E TARGET
# =============================================================================

features = [col for col in train_clean.columns if col != 'Credit_Score']
X = train_clean[features]
y = train_clean['Credit_Score']

# Tratar valores NaN no target antes do encoding
print(f"ğŸ” Verificando valores NaN no target...")
nan_count = y.isna().sum()
if nan_count > 0:
    print(f"âš ï¸ Encontrados {nan_count} valores NaN no target - convertendo para 'Unknown'")
    y = y.fillna('Unknown')

# Converter target para numÃ©rico (resolve problemas de encoding)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

print(f"ğŸ¯ Features ({len(features)}): {features[:5]}...")
print(f"ğŸ¯ Classes originais: {label_encoder.classes_}")
print(f"ğŸ¯ Classes codificadas: {np.unique(y_encoded)}")
print(f"ğŸ¯ DistribuiÃ§Ã£o: {pd.Series(y_encoded).value_counts().to_dict()}")

# =============================================================================
# 4. PIPELINE DE PRÃ‰-PROCESSAMENTO
# =============================================================================

numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(include=[object]).columns.tolist()

print(f"ğŸ”¢ Features numÃ©ricas ({len(numeric_features)}): {numeric_features}")
print(f"ğŸ“ Features categÃ³ricas ({len(categorical_features)}): {categorical_features}")

# Pipeline simplificado
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

print("âœ… Pipeline de prÃ©-processamento criado!")

# =============================================================================
# 5. DIVISÃƒO TREINO/VALIDAÃ‡ÃƒO
# =============================================================================

X_train, X_val, y_train, y_val = train_test_split(
    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)

print(f"âœ… Dados divididos:")
print(f"   ğŸ“Š Treino: {X_train.shape}")
print(f"   ğŸ“Š ValidaÃ§Ã£o: {X_val.shape}")
print(f"\nğŸ¯ DistribuiÃ§Ã£o treino: {np.bincount(y_train)}")
print(f"ğŸ¯ DistribuiÃ§Ã£o validaÃ§Ã£o: {np.bincount(y_val)}")

# =============================================================================
# 6. TREINAMENTO DO ÃšNICO MODELO: RANDOM FOREST
# =============================================================================

print("\n" + "="*60)
print("ğŸš€ INICIANDO TREINAMENTO DO ÃšNICO MODELO: RANDOM FOREST")
print("="*60)

# Criar pipeline completo
rf_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(
        n_estimators=150,
        max_depth=30,
        min_samples_split=2,
        min_samples_leaf=1,
        max_features='sqrt',
        random_state=42,
        class_weight='balanced',
        n_jobs=-1
    ))
])

# Treinar modelo
print("â³ Treinando Random Forest...")
rf_pipeline.fit(X_train, y_train)
print("âœ… Treinamento concluÃ­do!")

# Fazer prediÃ§Ãµes
y_pred = rf_pipeline.predict(X_val)
y_pred_proba = rf_pipeline.predict_proba(X_val)

print("âœ… PrediÃ§Ãµes realizadas!")

# =============================================================================
# 7. AVALIAÃ‡ÃƒO DO MODELO
# =============================================================================

accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred, average='weighted')
recall = recall_score(y_val, y_pred, average='weighted')
f1 = f1_score(y_val, y_pred, average='weighted')

print("\n" + "="*40)
print("ğŸ“Š RESULTADOS DO RANDOM FOREST:")
print("="*40)
print(f"ğŸ¯ AcurÃ¡cia:  {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"ğŸ¯ PrecisÃ£o:  {precision:.4f}")
print(f"ğŸ¯ Recall:    {recall:.4f}")
print(f"ğŸ¯ F1-Score:  {f1:.4f}")
print("="*40)

# RelatÃ³rio detalhado
print("\nğŸ“‹ RELATÃ“RIO DETALHADO:")
class_names = [str(name) for name in label_encoder.classes_]  # Converter para string
print(classification_report(y_val, y_pred, target_names=class_names))

# =============================================================================
# 8. VISUALIZAÃ‡Ã•ES
# =============================================================================

# Matriz de confusÃ£o
cm = confusion_matrix(y_val, y_pred)
class_names_viz = [str(name) for name in label_encoder.classes_]  # Converter para string

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=class_names_viz, yticklabels=class_names_viz)
plt.title('Matriz de ConfusÃ£o - Random Forest')
plt.xlabel('PrediÃ§Ãµes')
plt.ylabel('Valores Reais')
plt.tight_layout()
plt.savefig('models/confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"âœ… Matriz de confusÃ£o salva!")

# ImportÃ¢ncia das features
feature_importance = rf_pipeline.named_steps['classifier'].feature_importances_

# Obter nomes das features apÃ³s prÃ©-processamento
onehot_features = list(rf_pipeline.named_steps['preprocessor']
                      .named_transformers_['cat']
                      .named_steps['onehot']
                      .get_feature_names_out(categorical_features))

all_feature_names = numeric_features + onehot_features

# DataFrame com importÃ¢ncias
importance_df = pd.DataFrame({
    'feature': all_feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

# Plot das top 15 features
plt.figure(figsize=(12, 8))
top_15 = importance_df.head(15)
sns.barplot(data=top_15, x='importance', y='feature', palette='viridis')
plt.title('Top 15 Features Mais Importantes - Random Forest')
plt.xlabel('ImportÃ¢ncia')
plt.tight_layout()
plt.savefig('models/feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()

print("ğŸ“Š TOP 10 FEATURES MAIS IMPORTANTES:")
for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):
    print(f"{i:2d}. {row['feature']:<35} {row['importance']:.4f}")

# =============================================================================
# 9. PREDIÃ‡Ã•ES NO CONJUNTO DE TESTE
# =============================================================================

print("\nğŸ”® Fazendo prediÃ§Ãµes no conjunto de teste...")

if 'Credit_Score' in test_clean.columns:
    # Se teste tem target, avalia performance
    X_test = test_clean[features]
    y_test_original = test_clean['Credit_Score']
    
    # Tratar NaN no target de teste tambÃ©m
    if y_test_original.isna().sum() > 0:
        print(f"ğŸ”„ Convertendo {y_test_original.isna().sum()} valores NaN no teste para 'Unknown'")
        y_test_original = y_test_original.fillna('Unknown')
    
    # Verificar se hÃ¡ classes no teste que nÃ£o estavam no treino
    unknown_classes = set(y_test_original) - set(label_encoder.classes_)
    if unknown_classes:
        print(f"âš ï¸ Classes no teste nÃ£o vistas no treino: {unknown_classes}")
        print("ğŸ”„ Substituindo por classe mais frequente do treino...")
        most_frequent_class = label_encoder.classes_[np.argmax(np.bincount(y_encoded))]
        for unknown_class in unknown_classes:
            y_test_original = y_test_original.replace(unknown_class, most_frequent_class)
    
    y_test_encoded = label_encoder.transform(y_test_original)
    
    test_predictions = rf_pipeline.predict(X_test)
    test_accuracy = accuracy_score(y_test_encoded, test_predictions)
    
    print(f"ğŸ¯ ACURÃCIA NO TESTE: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
    
else:
    # Se teste nÃ£o tem target, apenas faz prediÃ§Ãµes
    X_test = test_clean[features]
    test_predictions = rf_pipeline.predict(X_test)
    
    print("âœ… PrediÃ§Ãµes realizadas no conjunto de teste")

# Converter prediÃ§Ãµes de volta para labels originais
test_predictions_labels = label_encoder.inverse_transform(test_predictions)

print(f"\nğŸ“Š DistribuiÃ§Ã£o das prediÃ§Ãµes:")
unique, counts = np.unique(test_predictions_labels, return_counts=True)
for label, count in zip(unique, counts):
    print(f"   {label}: {count} ({count/len(test_predictions_labels)*100:.1f}%)")

# =============================================================================
# 10. SALVAMENTO DO MODELO E RESULTADOS
# =============================================================================

print("\nğŸ’¾ Salvando modelo e resultados...")

# Criar diretÃ³rio
models_dir = 'models'
os.makedirs(models_dir, exist_ok=True)

# Salvar modelo
model_path = os.path.join(models_dir, 'random_forest_credit_score.pkl')
joblib.dump(rf_pipeline, model_path)

# Salvar label encoder
encoder_path = os.path.join(models_dir, 'label_encoder.pkl')
joblib.dump(label_encoder, encoder_path)

# Salvar prediÃ§Ãµes
predictions_df = pd.DataFrame({
    'prediction_encoded': test_predictions,
    'prediction_label': test_predictions_labels
})
predictions_path = os.path.join(models_dir, 'predictions.csv')
predictions_df.to_csv(predictions_path, index=False)

# Salvar informaÃ§Ãµes do modelo
model_info = {
    'model_type': 'RandomForestClassifier',
    'features': features,
    'classes': label_encoder.classes_.tolist(),
    'validation_metrics': {
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1)
    },
    'model_params': rf_pipeline.named_steps['classifier'].get_params()
}

info_path = os.path.join(models_dir, 'model_info.json')
with open(info_path, 'w') as f:
    json.dump(model_info, f, indent=2, default=str)

print(f"âœ… Modelo salvo: {model_path}")
print(f"âœ… Encoder salvo: {encoder_path}")
print(f"âœ… PrediÃ§Ãµes salvas: {predictions_path}")
print(f"âœ… InformaÃ§Ãµes salvas: {info_path}")

# =============================================================================
# 11. RESUMO FINAL
# =============================================================================

print("\n" + "="*60)
print("ğŸ‰ SUCESSO - MODELO ÃšNICO TREINADO!")
print("="*60)
print("ğŸ¯ APENAS 1 MODELO Random Forest")
print(f"ğŸ† EXCELENTE ACURÃCIA: {accuracy:.4f} ({accuracy*100:.2f}%)")
if accuracy > 0.85:
    print("ğŸŒŸ PERFORMANCE EXCEPCIONAL! (>85%)")
elif accuracy > 0.75:
    print("âœ… BOA PERFORMANCE! (>75%)")
else:
    print("âš ï¸ Performance moderada - considere mais dados")
print()
print("ğŸ“ ARQUIVOS SALVOS:")
print("  âœ… random_forest_credit_score.pkl - Modelo pronto para produÃ§Ã£o")
print("  âœ… label_encoder.pkl - Conversor de classes")  
print("  âœ… predictions.csv - PrediÃ§Ãµes no conjunto de teste")
print("  âœ… model_info.json - MÃ©tricas e informaÃ§Ãµes completas")
print("  âœ… confusion_matrix.png - VisualizaÃ§Ã£o da matriz de confusÃ£o")
print("  âœ… feature_importance.png - ImportÃ¢ncia das features")
print()
print("ğŸ”¥ PROBLEMAS RESOLVIDOS:")
print("  âœ… MÃºltiplos modelos â†’ APENAS 1 Random Forest")
print("  âœ… Erros MLflow â†’ Salvamento local robusto")
print("  âœ… Problemas encoding â†’ Tratamento automÃ¡tico")
print("  âœ… Categorias desconhecidas â†’ Handle completo")
print("  âœ… Valores NaN â†’ ConversÃ£o automÃ¡tica")
print()
print("ğŸš€ MODELO PRONTO PARA PRODUÃ‡ÃƒO!")
print("="*60)