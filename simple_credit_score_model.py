# MODELO √öNICO DE CREDIT SCORE - RANDOM FOREST
# Este script treina APENAS 1 MODELO Random Forest para classifica√ß√£o de score de cr√©dito

import pandas as pd
import numpy as np
import joblib
import os
import json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ Bibliotecas carregadas - APENAS para Random Forest!")

# =============================================================================
# 1. CARREGAMENTO DOS DADOS
# =============================================================================

train_df = pd.read_csv('references/exemplo_train.csv')
test_df = pd.read_csv('references/exemplo_test.csv')

print(f"üìä Dados de treino: {train_df.shape}")
print(f"üìä Dados de teste: {test_df.shape}")
print(f"\nüéØ Classes √∫nicas no target: {train_df['Credit_Score'].unique()}")

# =============================================================================
# 2. PR√â-PROCESSAMENTO SIMPLES E EFICAZ
# =============================================================================

def preprocess_simple(df):
    """Pr√©-processamento simplificado para evitar erros"""
    df_clean = df.copy()
    
    # Remove colunas desnecess√°rias
    cols_to_drop = ['ID', 'Customer_ID', 'Name', 'SSN']
    df_clean = df_clean.drop(columns=[col for col in cols_to_drop if col in df_clean.columns])
    
    # Converte colunas num√©ricas
    numeric_cols = ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 
                   'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date',
                   'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries',
                   'Outstanding_Debt', 'Credit_Utilization_Ratio', 'Total_EMI_per_month',
                   'Amount_invested_monthly', 'Monthly_Balance']
    
    for col in numeric_cols:
        if col in df_clean.columns:
            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
    
    # Limpa valores problem√°ticos em colunas categ√≥ricas
    categorical_cols = ['Month', 'Occupation', 'Type_of_Loan', 'Credit_Mix', 
                       'Credit_History_Age', 'Payment_of_Min_Amount', 'Payment_Behaviour']
    
    for col in categorical_cols:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].astype(str)
            df_clean[col] = df_clean[col].replace(['_', '!@9#%8', '#F%$D@*&8', '_______', 'NA', 'nan'], 'Unknown')
    
    return df_clean

# Aplica pr√©-processamento
train_clean = preprocess_simple(train_df)
test_clean = preprocess_simple(test_df)

print("‚úÖ Pr√©-processamento conclu√≠do!")
print(f"üìä Treino limpo: {train_clean.shape}")
print(f"üìä Teste limpo: {test_clean.shape}")

# =============================================================================
# 3. PREPARA√á√ÉO DAS FEATURES E TARGET
# =============================================================================

features = [col for col in train_clean.columns if col != 'Credit_Score']
X = train_clean[features]
y = train_clean['Credit_Score']

# Converter target para num√©rico (resolve problemas de encoding)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

print(f"üéØ Features ({len(features)}): {features[:5]}...")
print(f"üéØ Classes originais: {label_encoder.classes_}")
print(f"üéØ Classes codificadas: {np.unique(y_encoded)}")
print(f"üéØ Distribui√ß√£o: {pd.Series(y_encoded).value_counts().to_dict()}")

# =============================================================================
# 4. PIPELINE DE PR√â-PROCESSAMENTO
# =============================================================================

numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(include=[object]).columns.tolist()

print(f"üî¢ Features num√©ricas ({len(numeric_features)}): {numeric_features}")
print(f"üìù Features categ√≥ricas ({len(categorical_features)}): {categorical_features}")

# Pipeline simplificado
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

print("‚úÖ Pipeline de pr√©-processamento criado!")

# =============================================================================
# 5. DIVIS√ÉO TREINO/VALIDA√á√ÉO
# =============================================================================

X_train, X_val, y_train, y_val = train_test_split(
    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)

print(f"‚úÖ Dados divididos:")
print(f"   üìä Treino: {X_train.shape}")
print(f"   üìä Valida√ß√£o: {X_val.shape}")
print(f"\nüéØ Distribui√ß√£o treino: {np.bincount(y_train)}")
print(f"üéØ Distribui√ß√£o valida√ß√£o: {np.bincount(y_val)}")

# =============================================================================
# 6. TREINAMENTO DO √öNICO MODELO: RANDOM FOREST
# =============================================================================

print("\n" + "="*60)
print("üöÄ INICIANDO TREINAMENTO DO √öNICO MODELO: RANDOM FOREST")
print("="*60)

# Criar pipeline completo
rf_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(
        n_estimators=150,
        max_depth=30,
        min_samples_split=2,
        min_samples_leaf=1,
        max_features='sqrt',
        random_state=42,
        class_weight='balanced',
        n_jobs=-1
    ))
])

# Treinar modelo
print("‚è≥ Treinando Random Forest...")
rf_pipeline.fit(X_train, y_train)
print("‚úÖ Treinamento conclu√≠do!")

# Fazer predi√ß√µes
y_pred = rf_pipeline.predict(X_val)
y_pred_proba = rf_pipeline.predict_proba(X_val)

print("‚úÖ Predi√ß√µes realizadas!")

# =============================================================================
# 7. AVALIA√á√ÉO DO MODELO
# =============================================================================

accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred, average='weighted')
recall = recall_score(y_val, y_pred, average='weighted')
f1 = f1_score(y_val, y_pred, average='weighted')

print("\n" + "="*40)
print("üìä RESULTADOS DO RANDOM FOREST:")
print("="*40)
print(f"üéØ Acur√°cia:  {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"üéØ Precis√£o:  {precision:.4f}")
print(f"üéØ Recall:    {recall:.4f}")
print(f"üéØ F1-Score:  {f1:.4f}")
print("="*40)

# Relat√≥rio detalhado
print("\nüìã RELAT√ìRIO DETALHADO:")
class_names = label_encoder.classes_
print(classification_report(y_val, y_pred, target_names=class_names))

# =============================================================================
# 8. VISUALIZA√á√ïES
# =============================================================================

# Matriz de confus√£o
cm = confusion_matrix(y_val, y_pred)
class_names = label_encoder.classes_

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=class_names, yticklabels=class_names)
plt.title('Matriz de Confus√£o - Random Forest')
plt.xlabel('Predi√ß√µes')
plt.ylabel('Valores Reais')
plt.tight_layout()
plt.savefig('models/confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"‚úÖ Matriz de confus√£o salva!")

# Import√¢ncia das features
feature_importance = rf_pipeline.named_steps['classifier'].feature_importances_

# Obter nomes das features ap√≥s pr√©-processamento
onehot_features = list(rf_pipeline.named_steps['preprocessor']
                      .named_transformers_['cat']
                      .named_steps['onehot']
                      .get_feature_names_out(categorical_features))

all_feature_names = numeric_features + onehot_features

# DataFrame com import√¢ncias
importance_df = pd.DataFrame({
    'feature': all_feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

# Plot das top 15 features
plt.figure(figsize=(12, 8))
top_15 = importance_df.head(15)
sns.barplot(data=top_15, x='importance', y='feature', palette='viridis')
plt.title('Top 15 Features Mais Importantes - Random Forest')
plt.xlabel('Import√¢ncia')
plt.tight_layout()
plt.savefig('models/feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()

print("üìä TOP 10 FEATURES MAIS IMPORTANTES:")
for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):
    print(f"{i:2d}. {row['feature']:<35} {row['importance']:.4f}")

# =============================================================================
# 9. PREDI√á√ïES NO CONJUNTO DE TESTE
# =============================================================================

print("\nüîÆ Fazendo predi√ß√µes no conjunto de teste...")

if 'Credit_Score' in test_clean.columns:
    # Se teste tem target, avalia performance
    X_test = test_clean[features]
    y_test_original = test_clean['Credit_Score']
    y_test_encoded = label_encoder.transform(y_test_original)
    
    test_predictions = rf_pipeline.predict(X_test)
    test_accuracy = accuracy_score(y_test_encoded, test_predictions)
    
    print(f"üéØ ACUR√ÅCIA NO TESTE: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
    
else:
    # Se teste n√£o tem target, apenas faz predi√ß√µes
    X_test = test_clean[features]
    test_predictions = rf_pipeline.predict(X_test)
    
    print("‚úÖ Predi√ß√µes realizadas no conjunto de teste")

# Converter predi√ß√µes de volta para labels originais
test_predictions_labels = label_encoder.inverse_transform(test_predictions)

print(f"\nüìä Distribui√ß√£o das predi√ß√µes:")
unique, counts = np.unique(test_predictions_labels, return_counts=True)
for label, count in zip(unique, counts):
    print(f"   {label}: {count} ({count/len(test_predictions_labels)*100:.1f}%)")

# =============================================================================
# 10. SALVAMENTO DO MODELO E RESULTADOS
# =============================================================================

print("\nüíæ Salvando modelo e resultados...")

# Criar diret√≥rio
models_dir = 'models'
os.makedirs(models_dir, exist_ok=True)

# Salvar modelo
model_path = os.path.join(models_dir, 'random_forest_credit_score.pkl')
joblib.dump(rf_pipeline, model_path)

# Salvar label encoder
encoder_path = os.path.join(models_dir, 'label_encoder.pkl')
joblib.dump(label_encoder, encoder_path)

# Salvar predi√ß√µes
predictions_df = pd.DataFrame({
    'prediction_encoded': test_predictions,
    'prediction_label': test_predictions_labels
})
predictions_path = os.path.join(models_dir, 'predictions.csv')
predictions_df.to_csv(predictions_path, index=False)

# Salvar informa√ß√µes do modelo
model_info = {
    'model_type': 'RandomForestClassifier',
    'features': features,
    'classes': label_encoder.classes_.tolist(),
    'validation_metrics': {
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1)
    },
    'model_params': rf_pipeline.named_steps['classifier'].get_params()
}

info_path = os.path.join(models_dir, 'model_info.json')
with open(info_path, 'w') as f:
    json.dump(model_info, f, indent=2, default=str)

print(f"‚úÖ Modelo salvo: {model_path}")
print(f"‚úÖ Encoder salvo: {encoder_path}")
print(f"‚úÖ Predi√ß√µes salvas: {predictions_path}")
print(f"‚úÖ Informa√ß√µes salvas: {info_path}")

# =============================================================================
# 11. RESUMO FINAL
# =============================================================================

print("\n" + "="*60)
print("‚úÖ RESUMO FINAL")
print("="*60)
print("### Modelo Treinado:")
print("- Algoritmo: Random Forest (√öNICO MODELO)")
print(f"- Acur√°cia: {accuracy:.4f} ({accuracy*100:.2f}%)")
print("- Features: Processamento autom√°tico de num√©ricas e categ√≥ricas")
print("- Classes: Convers√£o autom√°tica string ‚Üî num√©rico")
print()
print("### Arquivos Gerados:")
print("1. random_forest_credit_score.pkl - Modelo treinado")
print("2. label_encoder.pkl - Conversor de classes")
print("3. predictions.csv - Predi√ß√µes no teste")
print("4. model_info.json - Informa√ß√µes do modelo")
print("5. confusion_matrix.png - Matriz de confus√£o")
print("6. feature_importance.png - Import√¢ncia das features")
print()
print("### ‚úÖ CONFIRMADO:")
print("APENAS 1 MODELO Random Forest foi treinado e salvo!")
print("Sem MLflow - Evita problemas de endpoint")
print("Sem m√∫ltiplos modelos - C√≥digo focado e limpo") 
print("Tratamento robusto - Resolve problemas de categorias e encoding")
print("="*60)