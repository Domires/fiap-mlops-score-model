{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Desenvolvimento do Modelo de Classificação de Score de Crédito\n",
        "\n",
        "Este notebook desenvolve um modelo de classificação para determinar o score de crédito dos clientes com base em suas características financeiras e comportamentais.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import dagshub\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import mlflow.sklearn\n",
        "import mlflow.catboost\n",
        "import mlflow.xgboost\n",
        "import mlflow.lightgbm\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix, roc_auc_score\n",
        ")\n",
        "from mlflow.models import infer_signature\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Carregamento dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregando dados de treino e teste\n",
        "train_df = pd.read_csv('../references/exemplo_train.csv')\n",
        "test_df = pd.read_csv('../references/exemplo_test.csv')\n",
        "\n",
        "print(f\"Dados de treino: {train_df.shape}\")\n",
        "print(f\"Dados de teste: {test_df.shape}\")\n",
        "\n",
        "# Visualizando primeiras linhas\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Exploração e Análise dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificando informações gerais\n",
        "print(\"Informações dos dados de treino:\")\n",
        "print(train_df.info())\n",
        "print(\"\\nValores únicos da variável alvo:\")\n",
        "print(train_df['Credit_Score'].value_counts())\n",
        "print(\"\\nDistribuição percentual:\")\n",
        "print(train_df['Credit_Score'].value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificando valores nulos\n",
        "print(\"Valores nulos por coluna:\")\n",
        "print(train_df.isnull().sum())\n",
        "print(\"\\nPercentual de valores nulos:\")\n",
        "print((train_df.isnull().sum() / len(train_df)) * 100)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Pré-processamento dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_data(df, is_training=True):\n",
        "    \"\"\"\n",
        "    Função para pré-processar os dados de credit score\n",
        "    \"\"\"\n",
        "    # Cópia do dataframe\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # Removendo colunas desnecessárias\n",
        "    columns_to_drop = ['ID', 'Customer_ID', 'Name', 'SSN']\n",
        "    df_processed = df_processed.drop(columns=[col for col in columns_to_drop if col in df_processed.columns])\n",
        "    \n",
        "    # Tratamento de valores problemáticos em colunas numéricas\n",
        "    numeric_columns = ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', \n",
        "                      'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date',\n",
        "                      'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries',\n",
        "                      'Outstanding_Debt', 'Credit_Utilization_Ratio', 'Total_EMI_per_month',\n",
        "                      'Amount_invested_monthly', 'Monthly_Balance']\n",
        "    \n",
        "    for col in numeric_columns:\n",
        "        if col in df_processed.columns:\n",
        "            # Convertendo valores não numéricos para NaN\n",
        "            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
        "            \n",
        "            # Tratando outliers negativos para idade\n",
        "            if col == 'Age':\n",
        "                df_processed[col] = df_processed[col].apply(lambda x: np.nan if x < 0 or x > 100 else x)\n",
        "    \n",
        "    # Tratamento de valores categóricos problemáticos\n",
        "    categorical_columns = ['Month', 'Occupation', 'Type_of_Loan', 'Credit_Mix', \n",
        "                          'Credit_History_Age', 'Payment_of_Min_Amount', 'Payment_Behaviour']\n",
        "    \n",
        "    for col in categorical_columns:\n",
        "        if col in df_processed.columns:\n",
        "            # Substituindo valores problemáticos por NaN\n",
        "            df_processed[col] = df_processed[col].replace(['_', '!@9#%8', '#F%$D@*&8', '_______', 'NA'], np.nan)\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "# Aplicando pré-processamento\n",
        "train_processed = preprocess_data(train_df, is_training=True)\n",
        "test_processed = preprocess_data(test_df, is_training=False)\n",
        "\n",
        "print(\"Dados após pré-processamento:\")\n",
        "print(f\"Treino: {train_processed.shape}\")\n",
        "print(f\"Teste: {test_processed.shape}\")\n",
        "print(\"\\nValores nulos após limpeza:\")\n",
        "print(train_processed.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Preparação das Features e Target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separando features e target\n",
        "features = list(train_processed.columns)\n",
        "features.remove('Credit_Score')\n",
        "\n",
        "X = train_processed[features]\n",
        "y = train_processed['Credit_Score']\n",
        "\n",
        "print(f\"Número de features: {len(features)}\")\n",
        "print(f\"Features: {features}\")\n",
        "print(f\"\\nDistribuição da variável alvo:\")\n",
        "print(y.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando pipeline de pré-processamento\n",
        "# Identificando colunas numéricas e categóricas\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=[object]).columns.tolist()\n",
        "\n",
        "print(f\"Features numéricas: {numeric_features}\")\n",
        "print(f\"Features categóricas: {categorical_features}\")\n",
        "\n",
        "# Pipeline de pré-processamento\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Dividindo dados para treino e validação\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTamanhos após divisão:\")\n",
        "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Configuração do MLflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuração do MLflow\n",
        "dagshub.init(repo_owner=\"domires\", repo_name=\"fiap-mlops-score-model\", mlflow=True)\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/domires/fiap-mlops-score-model.mlflow\")\n",
        "mlflow.autolog()\n",
        "\n",
        "print(\"MLflow configurado!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Função de Avaliação de Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_and_log_classification_model(kind, model_name, pipeline, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Função para avaliar e registrar modelos de classificação\n",
        "    \"\"\"\n",
        "    # Fazendo predições\n",
        "    predictions = pipeline.predict(X_val)\n",
        "    prediction_proba = pipeline.predict_proba(X_val) if hasattr(pipeline, \"predict_proba\") else None\n",
        "    \n",
        "    # Calculando métricas\n",
        "    accuracy = accuracy_score(y_val, predictions)\n",
        "    precision = precision_score(y_val, predictions, average='weighted')\n",
        "    recall = recall_score(y_val, predictions, average='weighted')\n",
        "    f1 = f1_score(y_val, predictions, average='weighted')\n",
        "    \n",
        "    # AUC-ROC para classificação multiclasse\n",
        "    if prediction_proba is not None:\n",
        "        try:\n",
        "            auc_roc = roc_auc_score(y_val, prediction_proba, multi_class='ovr', average='weighted')\n",
        "        except:\n",
        "            auc_roc = 0.0\n",
        "    else:\n",
        "        auc_roc = 0.0\n",
        "    \n",
        "    # Registrando métricas no MLflow\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"precision\", precision)\n",
        "    mlflow.log_metric(\"recall\", recall)\n",
        "    mlflow.log_metric(\"f1_score\", f1)\n",
        "    mlflow.log_metric(\"auc_roc\", auc_roc)\n",
        "    \n",
        "    # Criando assinatura do modelo\n",
        "    signature = infer_signature(X_val, predictions)\n",
        "    \n",
        "    # Registrando modelo baseado no tipo\n",
        "    if kind == \"catboost\":\n",
        "        mlflow.catboost.log_model(pipeline.named_steps['classifier'], model_name, signature=signature, input_example=X_val[:5])\n",
        "    elif kind == \"xgboost\":\n",
        "        mlflow.xgboost.log_model(pipeline.named_steps['classifier'], model_name, signature=signature, input_example=X_val[:5])\n",
        "    elif kind == \"lightgbm\":\n",
        "        mlflow.lightgbm.log_model(pipeline.named_steps['classifier'], model_name, signature=signature, input_example=X_val[:5])\n",
        "    else:\n",
        "        mlflow.sklearn.log_model(pipeline, model_name, signature=signature, input_example=X_val[:5])\n",
        "    \n",
        "    print(f\"Modelo {model_name} avaliado:\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1-Score: {f1:.4f}\")\n",
        "    print(f\"  AUC-ROC: {auc_roc:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'auc_roc': auc_roc\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Experimento 1: Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"Logistic Regression - Credit Score\"):\n",
        "    # Criando pipeline\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
        "    ])\n",
        "    \n",
        "    # Definindo parâmetros para busca\n",
        "    param_grid = {\n",
        "        'classifier__C': [0.1, 1.0, 10.0],\n",
        "        'classifier__solver': ['liblinear', 'lbfgs']\n",
        "    }\n",
        "    \n",
        "    # Grid search\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline, param_grid, cv=5, \n",
        "        scoring='accuracy', n_jobs=-1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    \n",
        "    # Melhor modelo\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    \n",
        "    # Registrando melhores parâmetros\n",
        "    mlflow.log_param(\"best_C\", grid_search.best_params_['classifier__C'])\n",
        "    mlflow.log_param(\"best_solver\", grid_search.best_params_['classifier__solver'])\n",
        "    \n",
        "    # Avaliando modelo\n",
        "    metrics = evaluate_and_log_classification_model(\"sklearn\", \"logistic_regression\", best_pipeline, X_val, y_val)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Experimento 2: Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"Random Forest - Credit Score\"):\n",
        "    # Criando pipeline\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(random_state=42))\n",
        "    ])\n",
        "    \n",
        "    # Definindo parâmetros para busca\n",
        "    param_grid = {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [10, 20, None],\n",
        "        'classifier__min_samples_split': [2, 5]\n",
        "    }\n",
        "    \n",
        "    # Grid search\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline, param_grid, cv=3, \n",
        "        scoring='accuracy', n_jobs=-1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    \n",
        "    # Melhor modelo\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    \n",
        "    # Registrando melhores parâmetros\n",
        "    mlflow.log_param(\"best_n_estimators\", grid_search.best_params_['classifier__n_estimators'])\n",
        "    mlflow.log_param(\"best_max_depth\", grid_search.best_params_['classifier__max_depth'])\n",
        "    mlflow.log_param(\"best_min_samples_split\", grid_search.best_params_['classifier__min_samples_split'])\n",
        "    \n",
        "    # Avaliando modelo\n",
        "    metrics = evaluate_and_log_classification_model(\"sklearn\", \"random_forest\", best_pipeline, X_val, y_val)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Experimento 3: XGBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"XGBoost - Credit Score\"):\n",
        "    # Criando pipeline\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
        "    ])\n",
        "    \n",
        "    # Definindo parâmetros para busca\n",
        "    param_grid = {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [3, 6],\n",
        "        'classifier__learning_rate': [0.1, 0.2]\n",
        "    }\n",
        "    \n",
        "    # Grid search\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline, param_grid, cv=3, \n",
        "        scoring='accuracy', n_jobs=-1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    \n",
        "    # Melhor modelo\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    \n",
        "    # Registrando melhores parâmetros\n",
        "    mlflow.log_param(\"best_n_estimators\", grid_search.best_params_['classifier__n_estimators'])\n",
        "    mlflow.log_param(\"best_max_depth\", grid_search.best_params_['classifier__max_depth'])\n",
        "    mlflow.log_param(\"best_learning_rate\", grid_search.best_params_['classifier__learning_rate'])\n",
        "    \n",
        "    # Avaliando modelo\n",
        "    metrics = evaluate_and_log_classification_model(\"xgboost\", \"xgboost_classifier\", best_pipeline, X_val, y_val)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Experimento 4: LightGBM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"LightGBM - Credit Score\"):\n",
        "    # Criando pipeline\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', lgb.LGBMClassifier(random_state=42, verbose=-1))\n",
        "    ])\n",
        "    \n",
        "    # Definindo parâmetros para busca\n",
        "    param_grid = {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [3, 6],\n",
        "        'classifier__learning_rate': [0.1, 0.2],\n",
        "        'classifier__num_leaves': [31, 50]\n",
        "    }\n",
        "    \n",
        "    # Grid search\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline, param_grid, cv=3, \n",
        "        scoring='accuracy', n_jobs=-1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    \n",
        "    # Melhor modelo\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    \n",
        "    # Registrando melhores parâmetros\n",
        "    mlflow.log_param(\"best_n_estimators\", grid_search.best_params_['classifier__n_estimators'])\n",
        "    mlflow.log_param(\"best_max_depth\", grid_search.best_params_['classifier__max_depth'])\n",
        "    mlflow.log_param(\"best_learning_rate\", grid_search.best_params_['classifier__learning_rate'])\n",
        "    mlflow.log_param(\"best_num_leaves\", grid_search.best_params_['classifier__num_leaves'])\n",
        "    \n",
        "    # Avaliando modelo\n",
        "    metrics = evaluate_and_log_classification_model(\"lightgbm\", \"lightgbm_classifier\", best_pipeline, X_val, y_val)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Avaliação Final no Conjunto de Teste\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aqui você pode carregar o melhor modelo do MLflow e avaliar no conjunto de teste\n",
        "# Exemplo de como preparar dados de teste quando disponível\n",
        "\n",
        "# X_test = test_processed[features]\n",
        "# # Se houver target no conjunto de teste:\n",
        "# # y_test = test_processed['Credit_Score']\n",
        "\n",
        "print(\"Desenvolvimento de modelos concluído!\")\n",
        "print(\"Verifique os resultados no MLflow UI para comparar os modelos.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Conclusão\n",
        "\n",
        "Este notebook desenvolveu e comparou diferentes modelos de classificação para score de crédito:\n",
        "\n",
        "1. **Logistic Regression** - Modelo linear baseline\n",
        "2. **Random Forest** - Ensemble de árvores de decisão\n",
        "3. **XGBoost** - Gradient boosting otimizado\n",
        "4. **LightGBM** - Gradient boosting eficiente\n",
        "\n",
        "Todas as métricas e modelos foram registrados no MLflow para comparação e seleção do melhor modelo.\n",
        "\n",
        "### Próximos Passos:\n",
        "1. Analisar os resultados no MLflow UI\n",
        "2. Selecionar o melhor modelo baseado nas métricas\n",
        "3. Registrar o modelo escolhido para produção\n",
        "4. Desenvolver a API para servir o modelo\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
